{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture-03 Gradient Descent and Dymanic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week, we need complete following tasks:\n",
    "+ Re-review the course online programming; \n",
    "+ Choose 1 - 2 books which you interested and keep reading; \n",
    "+ Answer the review questions\n",
    "+ Prepare the basic requirement of our 1st project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I Review the online programming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: change loss function from $loss = \\frac{1}{n}\\sum{(y_i - \\hat(y_i))^2}$ to $loss = \\frac{1}{n}\\sum{|y_i - \\hat{y_i}|}$, and using your mathmatical knowledge to get the right partial formual. Implemen the gradient descent code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#重新计算损失函数与倒数\n",
    "def loss2(y, y_hat): # to evaluate the performance \n",
    "    return sum(np.abs([(y_i - y_hat_i) for y_i, y_hat_i in zip(list(y), list(y_hat))]))/ len(list(y))\n",
    "def partial_k(x, y, y_hat):\n",
    "    n = len(y)\n",
    "\n",
    "    gradient = 0\n",
    "    for x_i, y_i, y_hat_i in zip(list(x), list(y), list(y_hat)):\n",
    "        # 两种情况\n",
    "        if( y_i-y_hat_i)>=0:\n",
    "            gradient += (-1) * x_i\n",
    "        else:\n",
    "            gradient=+x_i\n",
    "\n",
    "    \n",
    "    return 1 / n * gradient\n",
    "\n",
    "\n",
    "def partial_b(x, y, y_hat):\n",
    "    n = len(y)\n",
    "\n",
    "    gradient = 0\n",
    "    \n",
    "    for x_i, y_i, y_hat_i in zip(list(x), list(y), list(y_hat)):\n",
    "        if y_i-y_hat_i>=0:\n",
    "            gradient += (-1) \n",
    "        else:\n",
    "            gradient=+1\n",
    "\n",
    "    \n",
    "    return 1 / n * gradient\n",
    "def price(rm, k, b):\n",
    "    \"\"\"f(x) = k * x + b\"\"\"\n",
    "    return k * rm + b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When time is : 0, get best_k: 84.59558907812672 best_b: -25.53313025707334, and the loss is: 483.586411557492\n",
      "When time is : 500, get best_k: 84.58963058010463 best_b: -25.534118399366662, and the loss is: 483.54797643363156\n",
      "When time is : 1000, get best_k: 84.58367208208253 best_b: -25.535106541659985, and the loss is: 483.509541309772\n",
      "When time is : 1500, get best_k: 84.57771358406043 best_b: -25.53609468395331, and the loss is: 483.4711061859118\n",
      "When time is : 2000, get best_k: 84.57175508603834 best_b: -25.537082826246632, and the loss is: 483.4326710620516\n",
      "When time is : 2500, get best_k: 84.56579658801624 best_b: -25.538070968539955, and the loss is: 483.39423593819174\n",
      "When time is : 3000, get best_k: 84.55983808999414 best_b: -25.53905911083328, and the loss is: 483.3558008143318\n",
      "When time is : 3500, get best_k: 84.55387959197205 best_b: -25.540047253126602, and the loss is: 483.31736569047223\n",
      "When time is : 4000, get best_k: 84.54792109394995 best_b: -25.541035395419925, and the loss is: 483.27893056661173\n",
      "When time is : 4500, get best_k: 84.54196259592786 best_b: -25.54202353771325, and the loss is: 483.2404954427522\n",
      "When time is : 5000, get best_k: 84.53600409790576 best_b: -25.543011680006572, and the loss is: 483.20206031889165\n",
      "When time is : 5500, get best_k: 84.53004559988366 best_b: -25.543999822299895, and the loss is: 483.1636251950324\n",
      "When time is : 6000, get best_k: 84.52408710186157 best_b: -25.54498796459322, and the loss is: 483.1251900711721\n",
      "When time is : 6500, get best_k: 84.51812860383947 best_b: -25.545976106886542, and the loss is: 483.0867549473127\n",
      "When time is : 7000, get best_k: 84.51217010581738 best_b: -25.546964249179865, and the loss is: 483.04831982345246\n",
      "When time is : 7500, get best_k: 84.50621160779528 best_b: -25.54795239147319, and the loss is: 483.00988469959265\n",
      "When time is : 8000, get best_k: 84.50025310977318 best_b: -25.548940533766512, and the loss is: 482.97144957573215\n",
      "When time is : 8500, get best_k: 84.49429461175109 best_b: -25.549928676059835, and the loss is: 482.93301445187274\n",
      "When time is : 9000, get best_k: 84.48833611372899 best_b: -25.55091681835316, and the loss is: 482.89457932801196\n",
      "When time is : 9500, get best_k: 84.4823776157069 best_b: -25.551904960646482, and the loss is: 482.8561442041528\n",
      "When time is : 10000, get best_k: 84.4764191176848 best_b: -25.552893102939805, and the loss is: 482.8177090802926\n",
      "When time is : 10500, get best_k: 84.4704606196627 best_b: -25.55388124523313, and the loss is: 482.7792739564326\n",
      "When time is : 11000, get best_k: 84.4645021216406 best_b: -25.554869387526452, and the loss is: 482.7408388325725\n",
      "When time is : 11500, get best_k: 84.45854362361851 best_b: -25.555857529819775, and the loss is: 482.70240370871255\n",
      "When time is : 12000, get best_k: 84.45258512559641 best_b: -25.5568456721131, and the loss is: 482.6639685848525\n",
      "When time is : 12500, get best_k: 84.44662662757432 best_b: -25.557833814406422, and the loss is: 482.6255334609925\n",
      "When time is : 13000, get best_k: 84.44066812955222 best_b: -25.558821956699745, and the loss is: 482.58709833713317\n",
      "When time is : 13500, get best_k: 84.43470963153013 best_b: -25.55981009899307, and the loss is: 482.54866321327336\n",
      "When time is : 14000, get best_k: 84.42875113350803 best_b: -25.560798241286392, and the loss is: 482.51022808941315\n",
      "When time is : 14500, get best_k: 84.42279263548593 best_b: -25.561786383579715, and the loss is: 482.47179296555316\n",
      "When time is : 15000, get best_k: 84.41683413746384 best_b: -25.56277452587304, and the loss is: 482.4333578416928\n",
      "When time is : 15500, get best_k: 84.41087563944174 best_b: -25.563762668166362, and the loss is: 482.39492271783337\n",
      "When time is : 16000, get best_k: 84.40491714141965 best_b: -25.564750810459685, and the loss is: 482.35648759397304\n",
      "When time is : 16500, get best_k: 84.39895864339755 best_b: -25.56573895275301, and the loss is: 482.3180524701131\n",
      "When time is : 17000, get best_k: 84.39300014537545 best_b: -25.566727095046332, and the loss is: 482.27961734625285\n",
      "When time is : 17500, get best_k: 84.38704164735336 best_b: -25.567715237339655, and the loss is: 482.2411822223933\n",
      "When time is : 18000, get best_k: 84.38108314933126 best_b: -25.56870337963298, and the loss is: 482.2027470985331\n",
      "When time is : 18500, get best_k: 84.37512465130916 best_b: -25.569691521926302, and the loss is: 482.1643119746731\n",
      "When time is : 19000, get best_k: 84.36916615328707 best_b: -25.570679664219625, and the loss is: 482.12587685081337\n",
      "When time is : 19500, get best_k: 84.36320765526497 best_b: -25.57166780651295, and the loss is: 482.08744172695305\n",
      "When time is : 20000, get best_k: 84.35724915724288 best_b: -25.572655948806272, and the loss is: 482.0490066030927\n",
      "When time is : 20500, get best_k: 84.35129065922078 best_b: -25.573644091099595, and the loss is: 482.01057147923393\n",
      "When time is : 21000, get best_k: 84.34533216119868 best_b: -25.57463223339292, and the loss is: 481.9721363553737\n",
      "When time is : 21500, get best_k: 84.33937366317659 best_b: -25.575620375686242, and the loss is: 481.9337012315139\n",
      "When time is : 22000, get best_k: 84.33341516515449 best_b: -25.576608517979565, and the loss is: 481.8952661076532\n",
      "When time is : 22500, get best_k: 84.3274566671324 best_b: -25.57759666027289, and the loss is: 481.85683098379417\n",
      "When time is : 23000, get best_k: 84.3214981691103 best_b: -25.578584802566212, and the loss is: 481.8183958599336\n",
      "When time is : 23500, get best_k: 84.3155396710882 best_b: -25.579572944859535, and the loss is: 481.77996073607375\n",
      "When time is : 24000, get best_k: 84.3095811730661 best_b: -25.58056108715286, and the loss is: 481.74152561221354\n",
      "When time is : 24500, get best_k: 84.30362267504401 best_b: -25.581549229446182, and the loss is: 481.7030904883541\n",
      "When time is : 25000, get best_k: 84.29766417702191 best_b: -25.582537371739505, and the loss is: 481.6646553644935\n",
      "When time is : 25500, get best_k: 84.29170567899982 best_b: -25.58352551403283, and the loss is: 481.6262202406345\n",
      "When time is : 26000, get best_k: 84.28574718097772 best_b: -25.584513656326152, and the loss is: 481.58778511677383\n",
      "When time is : 26500, get best_k: 84.27978868295563 best_b: -25.585501798619475, and the loss is: 481.54934999291356\n",
      "When time is : 27000, get best_k: 84.27383018493353 best_b: -25.5864899409128, and the loss is: 481.51091486905403\n",
      "When time is : 27500, get best_k: 84.26787168691143 best_b: -25.587478083206122, and the loss is: 481.47247974519394\n",
      "When time is : 28000, get best_k: 84.26191318888934 best_b: -25.588466225499445, and the loss is: 481.4340446213339\n",
      "When time is : 28500, get best_k: 84.25595469086724 best_b: -25.58945436779277, and the loss is: 481.39560949747437\n",
      "When time is : 29000, get best_k: 84.24999619284515 best_b: -25.590442510086092, and the loss is: 481.3571743736142\n",
      "When time is : 29500, get best_k: 84.24403769482305 best_b: -25.591430652379415, and the loss is: 481.3187392497542\n",
      "When time is : 30000, get best_k: 84.23807919680095 best_b: -25.59241879467274, and the loss is: 481.2803041258947\n",
      "When time is : 30500, get best_k: 84.23212069877886 best_b: -25.593406936966062, and the loss is: 481.2418690020347\n",
      "When time is : 31000, get best_k: 84.22616220075676 best_b: -25.594395079259385, and the loss is: 481.2034338781742\n",
      "When time is : 31500, get best_k: 84.22020370273466 best_b: -25.59538322155271, and the loss is: 481.1649987543148\n",
      "When time is : 32000, get best_k: 84.21424520471257 best_b: -25.596371363846032, and the loss is: 481.1265636304544\n",
      "When time is : 32500, get best_k: 84.20828670669047 best_b: -25.597359506139355, and the loss is: 481.0881285065949\n",
      "When time is : 33000, get best_k: 84.20232820866838 best_b: -25.59834764843268, and the loss is: 481.04969338273486\n",
      "When time is : 33500, get best_k: 84.19636971064628 best_b: -25.599335790726002, and the loss is: 481.01125825887505\n",
      "When time is : 34000, get best_k: 84.19041121262418 best_b: -25.600323933019325, and the loss is: 480.9728231350149\n",
      "When time is : 34500, get best_k: 84.18445271460209 best_b: -25.60131207531265, and the loss is: 480.93438801115485\n",
      "When time is : 35000, get best_k: 84.17849421657999 best_b: -25.602300217605972, and the loss is: 480.89595288729487\n",
      "When time is : 35500, get best_k: 84.1725357185579 best_b: -25.603288359899295, and the loss is: 480.8575177634347\n",
      "When time is : 36000, get best_k: 84.1665772205358 best_b: -25.60427650219262, and the loss is: 480.81908263957484\n",
      "When time is : 36500, get best_k: 84.1606187225137 best_b: -25.605264644485942, and the loss is: 480.7806475157149\n",
      "When time is : 37000, get best_k: 84.15466022449161 best_b: -25.606252786779265, and the loss is: 480.74221239185505\n",
      "When time is : 37500, get best_k: 84.14870172646951 best_b: -25.60724092907259, and the loss is: 480.7037772679954\n",
      "When time is : 38000, get best_k: 84.14274322844742 best_b: -25.608229071365912, and the loss is: 480.66534214413485\n",
      "When time is : 38500, get best_k: 84.13678473042532 best_b: -25.609217213659235, and the loss is: 480.62690702027464\n",
      "When time is : 39000, get best_k: 84.13082623240322 best_b: -25.61020535595256, and the loss is: 480.5884718964154\n",
      "When time is : 39500, get best_k: 84.12486773438113 best_b: -25.611193498245882, and the loss is: 480.55003677255553\n",
      "When time is : 40000, get best_k: 84.11890923635903 best_b: -25.612181640539205, and the loss is: 480.51160164869515\n",
      "When time is : 40500, get best_k: 84.11295073833693 best_b: -25.61316978283253, and the loss is: 480.4731665248347\n",
      "When time is : 41000, get best_k: 84.10699224031484 best_b: -25.614157925125852, and the loss is: 480.43473140097575\n",
      "When time is : 41500, get best_k: 84.10103374229274 best_b: -25.615146067419175, and the loss is: 480.3962962771149\n",
      "When time is : 42000, get best_k: 84.09507524427065 best_b: -25.6161342097125, and the loss is: 480.35786115325556\n",
      "When time is : 42500, get best_k: 84.08911674624855 best_b: -25.617122352005822, and the loss is: 480.3194260293954\n",
      "When time is : 43000, get best_k: 84.08315824822645 best_b: -25.618110494299145, and the loss is: 480.28099090553553\n",
      "When time is : 43500, get best_k: 84.07719975020436 best_b: -25.61909863659247, and the loss is: 480.24255578167543\n",
      "When time is : 44000, get best_k: 84.07124125218226 best_b: -25.620086778885792, and the loss is: 480.20412065781625\n",
      "When time is : 44500, get best_k: 84.06528275416017 best_b: -25.621074921179115, and the loss is: 480.1656855339561\n",
      "When time is : 45000, get best_k: 84.05932425613807 best_b: -25.62206306347244, and the loss is: 480.1272504100954\n",
      "When time is : 45500, get best_k: 84.05336575811597 best_b: -25.623051205765762, and the loss is: 480.0888152862357\n",
      "When time is : 46000, get best_k: 84.04740726009388 best_b: -25.624039348059085, and the loss is: 480.05038016237563\n",
      "When time is : 46500, get best_k: 84.04144876207178 best_b: -25.62502749035241, and the loss is: 480.011945038516\n",
      "When time is : 47000, get best_k: 84.03549026404968 best_b: -25.626015632645732, and the loss is: 479.9735099146561\n",
      "When time is : 47500, get best_k: 84.02953176602759 best_b: -25.627003774939055, and the loss is: 479.93507479079614\n",
      "When time is : 48000, get best_k: 84.02357326800549 best_b: -25.62799191723238, and the loss is: 479.8966396669357\n",
      "When time is : 48500, get best_k: 84.0176147699834 best_b: -25.628980059525702, and the loss is: 479.8582045430748\n",
      "When time is : 49000, get best_k: 84.0116562719613 best_b: -25.629968201819025, and the loss is: 479.8197694192162\n",
      "When time is : 49500, get best_k: 84.0056977739392 best_b: -25.63095634411235, and the loss is: 479.7813342953561\n",
      "When time is : 50000, get best_k: 83.99973927591711 best_b: -25.631944486405672, and the loss is: 479.7428991714965\n",
      "When time is : 50500, get best_k: 83.99378077789501 best_b: -25.632932628698995, and the loss is: 479.7044640476368\n",
      "When time is : 51000, get best_k: 83.98782227987292 best_b: -25.63392077099232, and the loss is: 479.66602892377614\n",
      "When time is : 51500, get best_k: 83.98186378185082 best_b: -25.634908913285642, and the loss is: 479.62759379991587\n",
      "When time is : 52000, get best_k: 83.97590528382872 best_b: -25.635897055578965, and the loss is: 479.58915867605594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When time is : 52500, get best_k: 83.96994678580663 best_b: -25.63688519787229, and the loss is: 479.5507235521958\n",
      "When time is : 53000, get best_k: 83.96398828778453 best_b: -25.637873340165612, and the loss is: 479.5122884283365\n",
      "When time is : 53500, get best_k: 83.95802978976243 best_b: -25.638861482458935, and the loss is: 479.473853304477\n",
      "When time is : 54000, get best_k: 83.95207129174034 best_b: -25.63984962475226, and the loss is: 479.43541818061647\n",
      "When time is : 54500, get best_k: 83.94611279371824 best_b: -25.640837767045582, and the loss is: 479.3969830567567\n",
      "When time is : 55000, get best_k: 83.94015429569615 best_b: -25.641825909338905, and the loss is: 479.3585479328968\n",
      "When time is : 55500, get best_k: 83.93419579767405 best_b: -25.64281405163223, and the loss is: 479.32011280903663\n",
      "When time is : 56000, get best_k: 83.92823729965195 best_b: -25.643802193925552, and the loss is: 479.2816776851772\n",
      "When time is : 56500, get best_k: 83.92227880162986 best_b: -25.644790336218875, and the loss is: 479.24324256131706\n",
      "When time is : 57000, get best_k: 83.91632030360776 best_b: -25.6457784785122, and the loss is: 479.2048074374568\n",
      "When time is : 57500, get best_k: 83.91036180558567 best_b: -25.646766620805522, and the loss is: 479.1663723135974\n",
      "When time is : 58000, get best_k: 83.90440330756357 best_b: -25.647754763098845, and the loss is: 479.12793718973666\n",
      "When time is : 58500, get best_k: 83.89844480954147 best_b: -25.64874290539217, and the loss is: 479.08950206587696\n",
      "When time is : 59000, get best_k: 83.89248631151938 best_b: -25.649731047685492, and the loss is: 479.05106694201675\n",
      "When time is : 59500, get best_k: 83.88652781349728 best_b: -25.650719189978815, and the loss is: 479.01263181815756\n",
      "When time is : 60000, get best_k: 83.88056931547519 best_b: -25.65170733227214, and the loss is: 478.97419669429655\n",
      "When time is : 60500, get best_k: 83.87461081745309 best_b: -25.652695474565462, and the loss is: 478.9357615704368\n",
      "When time is : 61000, get best_k: 83.86865231943099 best_b: -25.653683616858785, and the loss is: 478.89732644657727\n",
      "When time is : 61500, get best_k: 83.8626938214089 best_b: -25.65467175915211, and the loss is: 478.8588913227171\n",
      "When time is : 62000, get best_k: 83.8567353233868 best_b: -25.655659901445432, and the loss is: 478.82045619885713\n",
      "When time is : 62500, get best_k: 83.8507768253647 best_b: -25.656648043738755, and the loss is: 478.78202107499715\n",
      "When time is : 63000, get best_k: 83.84481832734261 best_b: -25.65763618603208, and the loss is: 478.74358595113745\n",
      "When time is : 63500, get best_k: 83.83885982932051 best_b: -25.658624328325402, and the loss is: 478.7051508272775\n",
      "When time is : 64000, get best_k: 83.83290133129842 best_b: -25.659612470618725, and the loss is: 478.6667157034175\n",
      "When time is : 64500, get best_k: 83.82694283327632 best_b: -25.66060061291205, and the loss is: 478.6282805795579\n",
      "When time is : 65000, get best_k: 83.82098433525422 best_b: -25.661588755205372, and the loss is: 478.5898454556974\n",
      "When time is : 65500, get best_k: 83.81502583723213 best_b: -25.662576897498695, and the loss is: 478.5514103318375\n",
      "When time is : 66000, get best_k: 83.80906733921003 best_b: -25.66356503979202, and the loss is: 478.5129752079778\n",
      "When time is : 66500, get best_k: 83.80310884118794 best_b: -25.664553182085342, and the loss is: 478.47454008411734\n",
      "When time is : 67000, get best_k: 83.79715034316584 best_b: -25.665541324378665, and the loss is: 478.43610496025786\n",
      "When time is : 67500, get best_k: 83.79119184514374 best_b: -25.66652946667199, and the loss is: 478.39766983639805\n",
      "When time is : 68000, get best_k: 83.78523334712165 best_b: -25.667517608965312, and the loss is: 478.35923471253744\n",
      "When time is : 68500, get best_k: 83.77927484909955 best_b: -25.668505751258635, and the loss is: 478.32079958867746\n",
      "When time is : 69000, get best_k: 83.77331635107745 best_b: -25.66949389355196, and the loss is: 478.2823644648178\n",
      "When time is : 69500, get best_k: 83.76735785305536 best_b: -25.670482035845282, and the loss is: 478.2439293409577\n",
      "When time is : 70000, get best_k: 83.76139935503326 best_b: -25.671470178138605, and the loss is: 478.20549421709785\n",
      "When time is : 70500, get best_k: 83.75544085701117 best_b: -25.67245832043193, and the loss is: 478.1670590932381\n",
      "When time is : 71000, get best_k: 83.74948235898907 best_b: -25.673446462725252, and the loss is: 478.1286239693781\n",
      "When time is : 71500, get best_k: 83.74352386096697 best_b: -25.674434605018575, and the loss is: 478.0901888455176\n",
      "When time is : 72000, get best_k: 83.73756536294488 best_b: -25.6754227473119, and the loss is: 478.0517537216576\n",
      "When time is : 72500, get best_k: 83.73160686492278 best_b: -25.676410889605222, and the loss is: 478.01331859779765\n",
      "When time is : 73000, get best_k: 83.72564836690069 best_b: -25.677399031898545, and the loss is: 477.9748834739379\n",
      "When time is : 73500, get best_k: 83.71968986887859 best_b: -25.67838717419187, and the loss is: 477.93644835007825\n",
      "When time is : 74000, get best_k: 83.7137313708565 best_b: -25.679375316485192, and the loss is: 477.89801322621827\n",
      "When time is : 74500, get best_k: 83.7077728728344 best_b: -25.680363458778515, and the loss is: 477.85957810235806\n",
      "When time is : 75000, get best_k: 83.7018143748123 best_b: -25.68135160107184, and the loss is: 477.82114297849853\n",
      "When time is : 75500, get best_k: 83.6958558767902 best_b: -25.682339743365162, and the loss is: 477.7827078546382\n",
      "When time is : 76000, get best_k: 83.68989737876811 best_b: -25.683327885658485, and the loss is: 477.7442727307776\n",
      "When time is : 76500, get best_k: 83.68393888074601 best_b: -25.68431602795181, and the loss is: 477.70583760691864\n",
      "When time is : 77000, get best_k: 83.67798038272392 best_b: -25.685304170245132, and the loss is: 477.6674024830581\n",
      "When time is : 77500, get best_k: 83.67202188470182 best_b: -25.686292312538455, and the loss is: 477.62896735919855\n",
      "When time is : 78000, get best_k: 83.66606338667972 best_b: -25.68728045483178, and the loss is: 477.5905322353377\n",
      "When time is : 78500, get best_k: 83.66010488865763 best_b: -25.688268597125102, and the loss is: 477.5520971114784\n",
      "When time is : 79000, get best_k: 83.65414639063553 best_b: -25.689256739418425, and the loss is: 477.5136619876184\n",
      "When time is : 79500, get best_k: 83.64818789261344 best_b: -25.69024488171175, and the loss is: 477.475226863759\n",
      "When time is : 80000, get best_k: 83.64222939459134 best_b: -25.691233024005072, and the loss is: 477.436791739899\n",
      "When time is : 80500, get best_k: 83.63627089656924 best_b: -25.692221166298395, and the loss is: 477.39835661603854\n",
      "When time is : 81000, get best_k: 83.63031239854715 best_b: -25.69320930859172, and the loss is: 477.35992149217884\n",
      "When time is : 81500, get best_k: 83.62435390052505 best_b: -25.694197450885042, and the loss is: 477.3214863683194\n",
      "When time is : 82000, get best_k: 83.61839540250295 best_b: -25.695185593178365, and the loss is: 477.28305124445893\n",
      "When time is : 82500, get best_k: 83.61243690448086 best_b: -25.69617373547169, and the loss is: 477.24461612059883\n",
      "When time is : 83000, get best_k: 83.60647840645876 best_b: -25.697161877765012, and the loss is: 477.2061809967389\n",
      "When time is : 83500, get best_k: 83.60051990843667 best_b: -25.698150020058335, and the loss is: 477.1677458728789\n",
      "When time is : 84000, get best_k: 83.59456141041457 best_b: -25.69913816235166, and the loss is: 477.12931074901934\n",
      "When time is : 84500, get best_k: 83.58860291239247 best_b: -25.700126304644982, and the loss is: 477.09087562515936\n",
      "When time is : 85000, get best_k: 83.58264441437038 best_b: -25.701114446938305, and the loss is: 477.0524405012995\n",
      "When time is : 85500, get best_k: 83.57668591634828 best_b: -25.70210258923163, and the loss is: 477.0140053774398\n",
      "When time is : 86000, get best_k: 83.57072741832619 best_b: -25.703090731524952, and the loss is: 476.9755702535789\n",
      "When time is : 86500, get best_k: 83.56476892030409 best_b: -25.704078873818275, and the loss is: 476.93713512971954\n",
      "When time is : 87000, get best_k: 83.558810422282 best_b: -25.7050670161116, and the loss is: 476.89870000585944\n",
      "When time is : 87500, get best_k: 83.5528519242599 best_b: -25.706055158404922, and the loss is: 476.8602648819992\n",
      "When time is : 88000, get best_k: 83.5468934262378 best_b: -25.707043300698246, and the loss is: 476.82182975813913\n",
      "When time is : 88500, get best_k: 83.5409349282157 best_b: -25.70803144299157, and the loss is: 476.7833946342795\n",
      "When time is : 89000, get best_k: 83.53497643019361 best_b: -25.709019585284892, and the loss is: 476.7449595104199\n",
      "When time is : 89500, get best_k: 83.52901793217151 best_b: -25.710007727578216, and the loss is: 476.70652438656015\n",
      "When time is : 90000, get best_k: 83.52305943414942 best_b: -25.71099586987154, and the loss is: 476.6680892626991\n",
      "When time is : 90500, get best_k: 83.51710093612732 best_b: -25.711984012164862, and the loss is: 476.62965413883944\n",
      "When time is : 91000, get best_k: 83.51114243810522 best_b: -25.712972154458186, and the loss is: 476.5912190149799\n",
      "When time is : 91500, get best_k: 83.50518394008313 best_b: -25.71396029675151, and the loss is: 476.55278389111936\n",
      "When time is : 92000, get best_k: 83.49922544206103 best_b: -25.714948439044832, and the loss is: 476.51434876725995\n",
      "When time is : 92500, get best_k: 83.49326694403894 best_b: -25.715936581338156, and the loss is: 476.4759136433998\n",
      "When time is : 93000, get best_k: 83.48730844601684 best_b: -25.71692472363148, and the loss is: 476.43747851954026\n",
      "When time is : 93500, get best_k: 83.48134994799474 best_b: -25.717912865924802, and the loss is: 476.39904339568005\n",
      "When time is : 94000, get best_k: 83.47539144997265 best_b: -25.718901008218126, and the loss is: 476.3606082718201\n",
      "When time is : 94500, get best_k: 83.46943295195055 best_b: -25.71988915051145, and the loss is: 476.3221731479598\n",
      "When time is : 95000, get best_k: 83.46347445392846 best_b: -25.720877292804772, and the loss is: 476.2837380241002\n",
      "When time is : 95500, get best_k: 83.45751595590636 best_b: -25.721865435098096, and the loss is: 476.2453029002401\n",
      "When time is : 96000, get best_k: 83.45155745788426 best_b: -25.72285357739142, and the loss is: 476.2068677763806\n",
      "When time is : 96500, get best_k: 83.44559895986217 best_b: -25.723841719684742, and the loss is: 476.1684326525204\n",
      "When time is : 97000, get best_k: 83.43964046184007 best_b: -25.724829861978066, and the loss is: 476.1299975286605\n",
      "When time is : 97500, get best_k: 83.43368196381797 best_b: -25.72581800427139, and the loss is: 476.0915624048008\n",
      "When time is : 98000, get best_k: 83.42772346579588 best_b: -25.726806146564712, and the loss is: 476.05312728094026\n",
      "When time is : 98500, get best_k: 83.42176496777378 best_b: -25.727794288858036, and the loss is: 476.01469215708045\n",
      "When time is : 99000, get best_k: 83.41580646975169 best_b: -25.72878243115136, and the loss is: 475.9762570332202\n",
      "When time is : 99500, get best_k: 83.40984797172959 best_b: -25.729770573444682, and the loss is: 475.9378219093604\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import random\n",
    "import numpy as np\n",
    "data = load_boston()\n",
    "X, y = data['data'], data['target']\n",
    "X_rm = X[:, 5]\n",
    "\n",
    "\n",
    "#.3. Gradient Descent to get optimal k* and *b\n",
    "\n",
    "trying_times = 100000\n",
    "\n",
    "X, y = data['data'], data['target']\n",
    "\n",
    "min_loss = float('inf') \n",
    "\n",
    "current_k = random.random() * 200 - 100\n",
    "current_b = random.random() * 200 - 100\n",
    "\n",
    "learning_rate = 1e-03\n",
    "\n",
    "\n",
    "update_time = 0\n",
    "\n",
    "for i in range(trying_times):\n",
    "    \n",
    "    price_by_k_and_b = [price(r, current_k, current_b) for r in X_rm]\n",
    "    \n",
    "    current_loss = loss2(y, price_by_k_and_b)\n",
    "    \n",
    "\n",
    "    if current_loss < min_loss: # performance became better\n",
    "        min_loss = current_loss\n",
    "        \n",
    "        if i % 500 == 0: \n",
    "            print('When time is : {}, get best_k: {} best_b: {}, and the loss is: {}'.format(i,current_k, current_b, min_loss))\n",
    "\n",
    "    k_gradient = partial_k(X_rm, y, price_by_k_and_b)\n",
    "    \n",
    "    b_gradient = partial_b(X_rm, y, price_by_k_and_b)\n",
    "    \n",
    "    current_k = current_k + (-1 * k_gradient) * learning_rate\n",
    "\n",
    "    current_b = current_b + (-1 * b_gradient) * learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Finish the Solution Parse Part of Edit-Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#思考中\n",
    "#@lru_cache(maxsize=2**10)\n",
    "def edit_distance(string1, string2):\n",
    "    \n",
    "    if len(string1) == 0: return len(string2)\n",
    "    if len(string2) == 0: return len(string1)\n",
    "    \n",
    "    tail_s1 = string1[-1]\n",
    "    tail_s2 = string2[-1]\n",
    "    \n",
    "    candidates = [\n",
    "        (edit_distance(string1[:-1], string2) + 1, 'DEL {}'.format(tail_s1)),  # string 1 delete tail\n",
    "        (edit_distance(string1, string2[:-1]) + 1, 'ADD {}'.format(tail_s2)),  # string 1 add tail of string2\n",
    "    ]\n",
    "    \n",
    "    if tail_s1 == tail_s2:\n",
    "        both_forward = (edit_distance(string1[:-1], string2[:-1]) + 0, '')\n",
    "    else:\n",
    "        both_forward = (edit_distance(string1[:-1], string2[:-1]) + 1, 'SUB {} => {}'.format(tail_s1, tail_s2))\n",
    "\n",
    "    candidates.append(both_forward)\n",
    "    \n",
    "    min_distance, operation = min(candidates, key=lambda x: x[0])\n",
    "    \n",
    "    solution[(string1, string2)] = operation \n",
    "    \n",
    "    return min_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution = {}\n",
    "edit_distance('jary', 'jerry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('j', 'j'): '',\n",
       " ('j', 'je'): 'ADD e',\n",
       " ('j', 'jer'): 'ADD r',\n",
       " ('j', 'jerr'): 'ADD r',\n",
       " ('j', 'jerry'): 'ADD y',\n",
       " ('ja', 'j'): 'DEL a',\n",
       " ('ja', 'je'): 'SUB a => e',\n",
       " ('ja', 'jer'): 'ADD r',\n",
       " ('ja', 'jerr'): 'ADD r',\n",
       " ('ja', 'jerry'): 'ADD y',\n",
       " ('jar', 'j'): 'DEL r',\n",
       " ('jar', 'je'): 'DEL r',\n",
       " ('jar', 'jer'): '',\n",
       " ('jar', 'jerr'): 'ADD r',\n",
       " ('jar', 'jerry'): 'ADD y',\n",
       " ('jary', 'j'): 'DEL y',\n",
       " ('jary', 'je'): 'DEL y',\n",
       " ('jary', 'jer'): 'DEL y',\n",
       " ('jary', 'jerr'): 'SUB y => r',\n",
       " ('jary', 'jerry'): ''}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_solutions(solutions: dict, string1: str, string2: str) -> str:\n",
    "    route = []\n",
    "    def helper(solutions: dict, string1: str, string2: str) -> None:\n",
    "        if not string1 and not string2: return\n",
    "        operation = solutions[(string1, string2)]\n",
    "        route.append('{k}: {v}'.format(k=(string1, string2), v=operation))\n",
    "        if operation.startswith('SUB') or operation == '':\n",
    "            helper(solutions, string1[:-1], string2[:-1])\n",
    "        elif operation.startswith('ADD'):\n",
    "            helper(solutions, string1, string2[:-1])\n",
    "        elif operation.startswith('DEL'):\n",
    "            helper(solutions, string1[:-1], string2)\n",
    "    helper(solutions, string1, string2)\n",
    "    route.reverse()\n",
    "    return '=>'.join(route) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"('j', 'j'): =>('ja', 'je'): SUB a => e=>('jar', 'jer'): =>('jar', 'jerr'): ADD r=>('jary', 'jerry'): \""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_solutions(solution, 'jary', 'jerry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 Choose 1 - 2 books to keep reading: \n",
    "\n",
    "+ SICP, Structure and Interpretation of Computer Programming. \n",
    "+ Introduction to Algorithms \n",
    "+ Artificial Intelligence A Modern Approach (3rd Edition) \n",
    "+ Code Complete 2 \n",
    "+ Programming Pearls \n",
    "+ Deep Learning\n",
    "+ 黑客与画家\n",
    "+ 数学之美\n",
    "+ Fluent Python\n",
    "+ Hands on Tensorflow\n",
    "+ Conference: NIPS_ ICML_ ICLR_ ACL_ AAAI\n",
    "\n",
    "> most books you may find in our github: https://github.com/Computing-Intelligence/References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5-1: review machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why do we use Derivative / Gredient to fit a target function?¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:沿着梯度的反方向，目标函数时下降的最快的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In the words 'Gredient Descent', what's the Gredient and what's the Descent?¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:梯度就是导数，descent means the value decrease by the direction of derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. What's the advantages of the 3rd gradient descent method compared to the previous methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:Ans:the effciency is high and almost every iteration can get the smaller loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Using the simple words to describe: What's the machine leanring.¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:Ans:learn the formula or rules from data by machine itself"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Part 5: Answer following questions: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why do we need dynamic programming? What's the difference of dynamic programming and previous talked `search` problme? \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ans：\n",
    "1.提高效率，和分治法类似,但是每一个subproblem 解决后被储存，再遇见相同的sub会直接调用之前的计算结果，大大提高效率；\n",
    "2. 1.in dynamic each step we will minimize cost from source and cost  to target.\n",
    "   2.与分治法不同的是，适合于用动态规划求解的问题，经分解得到子问题往往不是互相独立的。若用分治法来解这类问题，则分解得到的子问题数目太多，有些子问题被重复计算了很多次。如果我们能够保存已解决的子问题的答案，而在需要时再找出已求得的答案，这样就可以避免大量的重复计算，节省时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Why do we still need dynamic programming? Why not we train a machine learning to fit a function which could get the `right` answer based on inputs?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " machine learning need lot data to train model; sometimes a new problem does not have such data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Can you catch up at least 3 problems which could solved by Dynamic Programming? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.edit distance ;\n",
    "2.有n级台阶，一个人每次上一级或者两级，问有多少种走完n级台阶的方法。\n",
    "3.给定数组arr，返回arr的最长递增子序列的长度，比如arr=[2,1,5,3,6,4,8,9,7]，最长递增子序列为[1,3,4,8,9]返回其长度为5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Can you catch up at least 3 problems wich could sloved by Edit Distance? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.DNA分析\n",
    "2.拼写纠错\n",
    "3.命名实体抽取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Please summarize the three main features of Dynamic Programming, and make a concise explain for each feature. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.overlapping subproblems:在计算过程中很多子问题是重复的\n",
    "2.overlapping computing saved in a table ：计算过的子问题，都将被存储，方便以后的调用\n",
    "3.parse solution： 解析出每次的最小单位子问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What's the disadvantages of Dynamic Programming? (You may need search by yourself in Internet)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. 没有统一的标准模型;\n",
    "2. 数值方法求解时存在维数灾。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6 Preparation of Project-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using python Flask or Bottle to finish your first simple web app:\n",
    "> https://bottlepy.org/\n",
    "\n",
    "2. Learn what's the SQL, and try some simple SQL operations:\n",
    "> https://www.w3schools.com/sql/sql_intro.asp\n",
    "\n",
    "3. Learn what's the HTML ( *ONLY* need to know the basic things)\n",
    "> https://getbootstrap.com/; https://www.w3schools.com/html/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optinal) Finish the k-person-salesman problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitudes = [random.randint(-100, 100) for _ in range(20)]\n",
    "longitude = [random.randint(-100, 100) for _ in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x17266ab7470>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(latitudes, longitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定一个初始点 𝑃, 已经 𝑘个车辆，如何从该点出发，经这 k 个车辆经过所以的点全部一次，而且所走过的路程最短?\n",
    "\n",
    "例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_p = (-50, 10)\n",
    "chosen_p2 = (1, 30)\n",
    "chosen_p3 = (99, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x17266b5f898>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(latitudes, longitude)\n",
    "plt.scatter([chosen_p[0]], [chosen_p[1]], color='r')\n",
    "plt.scatter([chosen_p2[0]], [chosen_p2[1]], color='r')\n",
    "plt.scatter([chosen_p3[0]], [chosen_p3[1]], color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
